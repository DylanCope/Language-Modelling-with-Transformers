{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physical Devices:\n",
      " [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')] \n",
      "\n",
      "\n",
      "Output Directory: ./outputs/20200324-000541\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.extend(['..'])\n",
    "\n",
    "from datetime import datetime \n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "import random\n",
    "import time\n",
    "from typing import Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import bernoulli\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "print('Physical Devices:\\n', tf.config.list_physical_devices(), '\\n')\n",
    "%load_ext tensorboard\n",
    "\n",
    "import transformers\n",
    "\n",
    "\n",
    "stamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# OUTPUTS_DIR = f'./outputs/{stamp}'\n",
    "OUTPUTS_DIR = './outputs/20200324-000541'\n",
    "print('\\nOutput Directory:', OUTPUTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./outputs/20200324-000541/logs\n"
     ]
    }
   ],
   "source": [
    "log_dir = f'{OUTPUTS_DIR}/logs'\n",
    "summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "tf.summary.trace_on(graph=True) \n",
    "print(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file(\n",
    "    'shakespeare.txt', \n",
    "    'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "shakespeare_lines = tf.data.TextLineDataset(path_to_file)\n",
    "shakespeare_tensor = tf.strings.join(list(iter(shakespeare_lines)),\n",
    "                                     separator='\\n')\n",
    "shakespeare_str = shakespeare_tensor.numpy().decode()\n",
    "print(shakespeare_str[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_SEQ_SIZE = 64\n",
    "MAX_SEQ_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def substr_generator():\n",
    "    text_size = len(shakespeare_str)\n",
    "    while True:\n",
    "        index = random.randint(0, text_size - MAX_SEQ_SIZE)\n",
    "        size = random.randint(MIN_SEQ_SIZE, MAX_SEQ_SIZE)\n",
    "        yield tf.strings.substr(shakespeare_tensor, index, size)\n",
    "\n",
    "random.seed(0)\n",
    "substrs_ds = tf.data.Dataset.from_generator(substr_generator, tf.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'alters.\\n\\nPERDITA:\\nOne of these is true:\\nI think affliction may subdue the cheek,\\nBut not take in the mind.\\n\\nCAMILLO:\\n', shape=(), dtype=string)\n",
      "tf.Tensor(b'w together: grant that, and tell me,\\nIn peace what each of them by the other lose,\\nThat they comb', shape=(), dtype=string)\n",
      "tf.Tensor(b'hs up,\\nAfter our great good cheer. Pray you, sit down;\\nFor now we sit to chat as well as eat.\\n\\nPETRUCHIO:\\nNothing but sit and ', shape=(), dtype=string)\n",
      "tf.Tensor(b'on\\nThat does affect it. Once more, fare you well.\\n\\nANGELO:\\nThe heavens give safety to your purposes!\\n\\n', shape=(), dtype=string)\n",
      "tf.Tensor(b'o do?\\n\\nPETRUCHIO:\\nNot her that chides, sir, at any hand, I pray.\\n\\nTRANIO:\\nI love no chiders, sir. Biondello, ', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for x in substrs_ds.take(5):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language = set(shakespeare_str)\n",
    "language_size = len(language)\n",
    "language_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 8\n",
    "hidden_size = 256\n",
    "num_heads = 8\n",
    "max_positions=128\n",
    "\n",
    "num_special_tokens = 2\n",
    "pad_token = language_size + 1\n",
    "mask_token = language_size + 2\n",
    "# mask hyperparams from https://arxiv.org/pdf/1810.04805.pdf\n",
    "prob_mask=0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_string(x: tf.Tensor) -> str:\n",
    "    def _convert(i: int) -> str:\n",
    "        if i == mask_token:\n",
    "            return '[MASK]'\n",
    "        if i == pad_token:\n",
    "            return '[PAD]'\n",
    "        return idx2char[i]\n",
    "    return ''.join(_convert(i) for i in x.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Trace already enabled\n"
     ]
    }
   ],
   "source": [
    "tf.summary.trace_on(graph=True, profiler=True)\n",
    "# model_config = transformers.BertConfig(\n",
    "#     vocab_size_or_config_json_file=language_size,\n",
    "#     type_vocab_size=num_special_tokens, \n",
    "#     hidden_size=hidden_size,\n",
    "#     intermediate_size=hidden_size*2,\n",
    "#     num_hidden_layers=num_layers,\n",
    "#     num_attention_heads=num_heads\n",
    "# )\n",
    "# model = transformers.TFBertForMaskedLM(model_config)\n",
    "model_config = transformers.OpenAIGPTConfig(\n",
    "    vocab_size_or_config_json_file=language_size,\n",
    "    type_vocab_size=num_special_tokens,\n",
    "    n_positions=max_positions,\n",
    "    n_ctx=max_positions,\n",
    "    n_embd=hidden_size,\n",
    "    n_layer=num_layers,\n",
    "    n_head=num_heads\n",
    ")\n",
    "model = transformers.TFOpenAIGPTLMHeadModel(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "train_acc = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint restored!!\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = f'{OUTPUTS_DIR}/ckpts'\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=model,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(language)}\n",
    "idx2char = dict(enumerate(language))\n",
    "\n",
    "\n",
    "def encode(text_tensor: tf.Tensor) -> List[List[int]]:\n",
    "    text_str = text_tensor.numpy().decode()\n",
    "    return [[char2idx[c] for c in text_str]]\n",
    "\n",
    "\n",
    "def tf_encode(text_tensor: tf.Tensor) -> tf.Tensor:\n",
    "    return tf.py_function(encode, [text_tensor], tf.int32)\n",
    "\n",
    "\n",
    "dataset = substrs_ds.map(tf_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(128,), dtype=int32, numpy=\n",
       "array([ 0, 17, 47, 24, 30, 64, 60, 64, 56, 51, 16, 57, 16, 64, 56, 16, 23,\n",
       "       13, 59, 13, 29, 24, 58, 23, 64, 15, 16, 14, 27, 16, 39, 63, 25, 59,\n",
       "       16, 50, 14, 42, 42, 20, 24, 24, 48,  5, 18, 54, 16, 19,  5, 53, 41,\n",
       "       48,  0, 17, 47, 24, 17, 23, 14, 27, 16, 14, 27, 16, 15, 23, 13, 16,\n",
       "       56, 64, 15, 15, 13, 59, 47,  8,  8,  9, 25, 59, 27, 13, 51, 16,  2,\n",
       "       14, 46, 13, 16, 42, 13, 64, 46, 13, 16, 64, 50, 23, 14, 42, 13, 51,\n",
       "       24, 58, 13, 16, 56, 25, 27, 15, 16, 15, 64, 42, 35, 16, 14,  3, 16,\n",
       "       27, 13, 11, 59, 13, 15, 47,  8,  8])>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask_and_input(tar: tf.Tensor) -> tf.Tensor:\n",
    "    where_masked = tf.random.uniform(tar.shape) < prob_mask\n",
    "    where_masked &= tar != pad_token\n",
    "    mask_tokens = tf.multiply(mask_token, tf.cast(where_masked, tf.int32))\n",
    "    not_masked = tf.multiply(tar, 1 - tf.cast(where_masked, tf.int32))\n",
    "    inp = mask_tokens + not_masked\n",
    "    return inp, where_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar = next(iter(dataset))\n",
    "inp, where_masked = create_mask_and_input(tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(100,), dtype=int32, numpy=\n",
       " array([14, 11, 23, 16,  3, 63, 50, 51, 16, 15, 50, 63, 16, 15, 13,  3, 60,\n",
       "        13, 59, 16,  7, 42, 64, 39, 37, 13, 42, 42, 63, 50, 27, 16, 15, 63,\n",
       "        16, 60, 25, 27, 15, 51, 24, 17, 23, 39, 16, 40, 59, 63, 35, 13,  3,\n",
       "        16, 37, 64, 14, 15, 23, 16, 23, 64, 15, 23, 16, 56, 64, 60, 13, 16,\n",
       "        64, 16,  7, 59, 13, 39, 16, 37, 63, 59, 16, 50, 63, 59, 56, 27, 29,\n",
       "        24, 58, 23, 64, 15, 16, 11, 64,  3, 27, 15, 16, 15, 23, 63])>,\n",
       " <tf.Tensor: shape=(100,), dtype=bool, numpy=\n",
       " array([False,  True, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False,  True,\n",
       "        False, False,  True, False, False, False,  True,  True, False,\n",
       "        False, False,  True, False,  True, False, False,  True, False,\n",
       "        False,  True,  True, False,  True, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "         True, False, False,  True, False,  True, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False,  True, False, False, False, False, False,  True,  True,\n",
       "         True])>,\n",
       " <tf.Tensor: shape=(100,), dtype=int32, numpy=\n",
       " array([14, 67, 23, 16,  3, 63, 50, 51, 16, 15, 50, 63, 16, 15, 13,  3, 60,\n",
       "        13, 59, 16,  7, 42, 64, 39, 37, 13, 42, 42, 63, 50, 27, 16, 15, 63,\n",
       "        16, 67, 25, 27, 67, 51, 24, 17, 67, 67, 16, 40, 59, 67, 35, 67,  3,\n",
       "        16, 67, 64, 14, 67, 67, 16, 67, 64, 15, 23, 16, 56, 64, 60, 13, 16,\n",
       "        64, 16,  7, 59, 67, 39, 16, 67, 63, 67, 16, 50, 63, 59, 56, 27, 29,\n",
       "        24, 58, 23, 64, 15, 16, 67, 64,  3, 27, 15, 16, 67, 67, 67])>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tar, where_masked, inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i[MASK]h now, two tender playfellows to [MASK]us[MASK],\n",
      "T[MASK][MASK] br[MASK]k[MASK]n [MASK]ai[MASK][MASK] [MASK]ath made a pr[MASK]y [MASK]o[MASK] worms.\n",
      "What [MASK]anst [MASK][MASK][MASK] \n",
      "\n",
      " ich now, two tender playfellows to dust,\n",
      "Thy broken faith hath made a prey for worms.\n",
      "What canst tho\n"
     ]
    }
   ],
   "source": [
    "print(to_string(inp), '\\n\\n', to_string(tar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, *_ = model(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ZDqxxZqZZqzZzqzzzz', 'cdthyoefthhefrctho')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_string(tf.argmax(logits[where_masked], 1)), to_string(tar[where_masked])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=6.1102123>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(tar[where_masked], logits[where_masked])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_open_aigptlm_head_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "transformer (TFOpenAIGPTMain multiple                  6367488   \n",
      "=================================================================\n",
      "Total params: 6,367,488\n",
      "Trainable params: 6,367,488\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "with summary_writer.as_default():\n",
    "    tf.summary.trace_export(\n",
    "      name=\"transformer\",\n",
    "      step=0, profiler_outdir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "train_dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.padded_batch(\n",
    "    BATCH_SIZE, \n",
    "    padded_shapes=[MAX_SEQ_SIZE], \n",
    "    padding_values=pad_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([8, 128]), TensorShape([8, 128]), TensorShape([8, 128]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tar = next(iter(train_dataset))\n",
    "inp, where_masked = create_mask_and_input(tar)\n",
    "tar.shape, inp.shape, where_masked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=4.2594767>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding_mask = tf.cast(inp != pad_token, tf.int32)\n",
    "logits, *_ = model(inp, attention_mask=padding_mask)\n",
    "loss_fn(tar[where_masked], logits[where_masked])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: tf.keras.Model,\n",
    "               optimizer: tf.keras.optimizers.Optimizer,\n",
    "               pad_token: int,\n",
    "               tar: tf.Tensor):\n",
    "    \n",
    "    inp, where_masked = create_mask_and_input(tar)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        attention_mask = tf.cast(inp != pad_token, tf.int32)\n",
    "        logits, *_ = model(inp, \n",
    "                           attention_mask=attention_mask,\n",
    "                           training=True)\n",
    "        loss = loss_fn(tar[where_masked], logits[where_masked])\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_acc(tar[where_masked], logits[where_masked])\n",
    "    train_loss(loss)\n",
    "    \n",
    "    return logits, loss\n",
    "\n",
    "\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(BATCH_SIZE, MAX_SEQ_SIZE), \n",
    "                  dtype=tf.int32),\n",
    "]\n",
    "\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step_tf(tar):\n",
    "    train_step(model, optimizer, pad_token, tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCHES_IN_EPOCH = 250\n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dylan\\Miniconda3\\envs\\xai-it\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "C:\\Users\\dylan\\Miniconda3\\envs\\xai-it\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 3.7273 Accuracy 0.1222\n",
      "Time taken for epoch: 89.79784440994263 secs\n",
      "\n",
      "Epoch 2 Loss 3.3987 Accuracy 0.1516\n",
      "Time taken for epoch: 72.9276659488678 secs\n",
      "\n",
      "Epoch 3 Loss 3.3596 Accuracy 0.1494\n",
      "Time taken for epoch: 73.06627106666565 secs\n",
      "\n",
      "Epoch 4 Loss 3.3281 Accuracy 0.1536\n",
      "Time taken for epoch: 73.49952435493469 secs\n",
      "\n",
      "Saving checkpoint for epoch 5 at ./outputs/20200324-000541/ckpts\\ckpt-1\n",
      "Epoch 5 Loss 3.3388 Accuracy 0.1502\n",
      "Time taken for epoch: 74.96120643615723 secs\n",
      "\n",
      "Epoch 6 Loss 3.3133 Accuracy 0.1533\n",
      "Time taken for epoch: 74.62173795700073 secs\n",
      "\n",
      "Epoch 7 Loss 3.3179 Accuracy 0.1531\n",
      "Time taken for epoch: 72.82323098182678 secs\n",
      "\n",
      "Epoch 8 Loss 3.3127 Accuracy 0.1536\n",
      "Time taken for epoch: 72.62923979759216 secs\n",
      "\n",
      "Epoch 9 Loss 3.3268 Accuracy 0.1528\n",
      "Time taken for epoch: 72.97048950195312 secs\n",
      "\n",
      "Saving checkpoint for epoch 10 at ./outputs/20200324-000541/ckpts\\ckpt-2\n",
      "Epoch 10 Loss 3.3090 Accuracy 0.1508\n",
      "Time taken for epoch: 74.00064945220947 secs\n",
      "\n",
      "Epoch 11 Loss 3.3143 Accuracy 0.1506\n",
      "Time taken for epoch: 74.3178358078003 secs\n",
      "\n",
      "Epoch 12 Loss 3.2967 Accuracy 0.1551\n",
      "Time taken for epoch: 73.2215211391449 secs\n",
      "\n",
      "Epoch 13 Loss 3.3194 Accuracy 0.1561\n",
      "Time taken for epoch: 73.41912245750427 secs\n",
      "\n",
      "Epoch 14 Loss 3.3160 Accuracy 0.1518\n",
      "Time taken for epoch: 73.68266987800598 secs\n",
      "\n",
      "Saving checkpoint for epoch 15 at ./outputs/20200324-000541/ckpts\\ckpt-3\n",
      "Epoch 15 Loss 3.3163 Accuracy 0.1511\n",
      "Time taken for epoch: 74.38850259780884 secs\n",
      "\n",
      "Epoch 16 Loss 3.3086 Accuracy 0.1506\n",
      "Time taken for epoch: 74.47801399230957 secs\n",
      "\n",
      "Epoch 17 Loss 3.2964 Accuracy 0.1550\n",
      "Time taken for epoch: 73.36191034317017 secs\n",
      "\n",
      "Epoch 18 Loss 3.3093 Accuracy 0.1525\n",
      "Time taken for epoch: 73.361093044281 secs\n",
      "\n",
      "Epoch 19 Loss 3.2985 Accuracy 0.1522\n",
      "Time taken for epoch: 75.47002744674683 secs\n",
      "\n",
      "Saving checkpoint for epoch 20 at ./outputs/20200324-000541/ckpts\\ckpt-4\n",
      "Epoch 20 Loss 3.3030 Accuracy 0.1523\n",
      "Time taken for epoch: 76.96724104881287 secs\n",
      "\n",
      "Epoch 21 Loss 3.2956 Accuracy 0.1541\n",
      "Time taken for epoch: 76.54950380325317 secs\n",
      "\n",
      "Epoch 22 Loss 3.3024 Accuracy 0.1524\n",
      "Time taken for epoch: 75.17398309707642 secs\n",
      "\n",
      "Epoch 23 Loss 3.2850 Accuracy 0.1527\n",
      "Time taken for epoch: 74.8006980419159 secs\n",
      "\n",
      "Epoch 24 Loss 3.2770 Accuracy 0.1520\n",
      "Time taken for epoch: 74.72382164001465 secs\n",
      "\n",
      "Saving checkpoint for epoch 25 at ./outputs/20200324-000541/ckpts\\ckpt-5\n",
      "Epoch 25 Loss 3.2651 Accuracy 0.1511\n",
      "Time taken for epoch: 76.75779914855957 secs\n",
      "\n",
      "Epoch 26 Loss 3.2411 Accuracy 0.1557\n",
      "Time taken for epoch: 75.9891254901886 secs\n",
      "\n",
      "Epoch 27 Loss 3.2370 Accuracy 0.1550\n",
      "Time taken for epoch: 74.14269852638245 secs\n",
      "\n",
      "Epoch 28 Loss 3.2159 Accuracy 0.1548\n",
      "Time taken for epoch: 74.95712113380432 secs\n",
      "\n",
      "Epoch 29 Loss 3.1968 Accuracy 0.1559\n",
      "Time taken for epoch: 75.85036063194275 secs\n",
      "\n",
      "Saving checkpoint for epoch 30 at ./outputs/20200324-000541/ckpts\\ckpt-6\n",
      "Epoch 30 Loss 3.1810 Accuracy 0.1573\n",
      "Time taken for epoch: 76.84894299507141 secs\n",
      "\n",
      "Epoch 31 Loss 3.1777 Accuracy 0.1593\n",
      "Time taken for epoch: 75.64625096321106 secs\n",
      "\n",
      "Epoch 32 Loss 3.1787 Accuracy 0.1565\n",
      "Time taken for epoch: 75.66703867912292 secs\n",
      "\n",
      "Epoch 33 Loss 3.1584 Accuracy 0.1554\n",
      "Time taken for epoch: 74.98759508132935 secs\n",
      "\n",
      "Epoch 34 Loss 3.1413 Accuracy 0.1589\n",
      "Time taken for epoch: 75.409255027771 secs\n",
      "\n",
      "Saving checkpoint for epoch 35 at ./outputs/20200324-000541/ckpts\\ckpt-7\n",
      "Epoch 35 Loss 3.1438 Accuracy 0.1580\n",
      "Time taken for epoch: 76.84470009803772 secs\n",
      "\n",
      "Epoch 36 Loss 3.1405 Accuracy 0.1615\n",
      "Time taken for epoch: 76.00297117233276 secs\n",
      "\n",
      "Epoch 37 Loss 3.1362 Accuracy 0.1620\n",
      "Time taken for epoch: 75.46830821037292 secs\n",
      "\n",
      "Epoch 38 Loss 3.1213 Accuracy 0.1640\n",
      "Time taken for epoch: 75.35190916061401 secs\n",
      "\n",
      "Epoch 39 Loss 3.1206 Accuracy 0.1651\n",
      "Time taken for epoch: 75.53845143318176 secs\n",
      "\n",
      "Saving checkpoint for epoch 40 at ./outputs/20200324-000541/ckpts\\ckpt-8\n",
      "Epoch 40 Loss 3.1201 Accuracy 0.1651\n",
      "Time taken for epoch: 76.76700139045715 secs\n",
      "\n",
      "Epoch 41 Loss 3.1127 Accuracy 0.1637\n",
      "Time taken for epoch: 76.82911610603333 secs\n",
      "\n",
      "Epoch 42 Loss 3.1004 Accuracy 0.1666\n",
      "Time taken for epoch: 83.57067465782166 secs\n",
      "\n",
      "Epoch 43 Loss 3.0918 Accuracy 0.1672\n",
      "Time taken for epoch: 74.63127565383911 secs\n",
      "\n",
      "Epoch 44 Loss 3.0735 Accuracy 0.1701\n",
      "Time taken for epoch: 74.02266335487366 secs\n",
      "\n",
      "Saving checkpoint for epoch 45 at ./outputs/20200324-000541/ckpts\\ckpt-9\n",
      "Epoch 45 Loss 3.0677 Accuracy 0.1714\n",
      "Time taken for epoch: 80.56965899467468 secs\n",
      "\n",
      "Epoch 46 Loss 3.0466 Accuracy 0.1743\n",
      "Time taken for epoch: 77.05281972885132 secs\n",
      "\n",
      "Epoch 47 Loss 3.0326 Accuracy 0.1793\n",
      "Time taken for epoch: 77.254469871521 secs\n",
      "\n",
      "Epoch 48 Loss 3.0303 Accuracy 0.1777\n",
      "Time taken for epoch: 75.89512181282043 secs\n",
      "\n",
      "Epoch 49 Loss 3.0147 Accuracy 0.1831\n",
      "Time taken for epoch: 77.00462865829468 secs\n",
      "\n",
      "Saving checkpoint for epoch 50 at ./outputs/20200324-000541/ckpts\\ckpt-10\n",
      "Epoch 50 Loss 2.9923 Accuracy 0.1852\n",
      "Time taken for epoch: 78.37576174736023 secs\n",
      "\n",
      "Epoch 51 Loss 2.9916 Accuracy 0.1869\n",
      "Time taken for epoch: 78.0270848274231 secs\n",
      "\n",
      "Epoch 52 Loss 2.9940 Accuracy 0.1882\n",
      "Time taken for epoch: 76.07028841972351 secs\n",
      "\n",
      "Epoch 53 Loss 2.9627 Accuracy 0.1951\n",
      "Time taken for epoch: 76.59826970100403 secs\n",
      "\n",
      "Epoch 54 Loss 2.9688 Accuracy 0.1910\n",
      "Time taken for epoch: 76.9966778755188 secs\n",
      "\n",
      "Saving checkpoint for epoch 55 at ./outputs/20200324-000541/ckpts\\ckpt-11\n",
      "Epoch 55 Loss 2.9498 Accuracy 0.1959\n",
      "Time taken for epoch: 78.19463753700256 secs\n",
      "\n",
      "Epoch 56 Loss 2.9457 Accuracy 0.1998\n",
      "Time taken for epoch: 77.28714060783386 secs\n",
      "\n",
      "Epoch 57 Loss 2.9435 Accuracy 0.1964\n",
      "Time taken for epoch: 76.03413891792297 secs\n",
      "\n",
      "Epoch 58 Loss 2.9374 Accuracy 0.2008\n",
      "Time taken for epoch: 76.38175392150879 secs\n",
      "\n",
      "Epoch 59 Loss 2.9128 Accuracy 0.2063\n",
      "Time taken for epoch: 76.2527289390564 secs\n",
      "\n",
      "Saving checkpoint for epoch 60 at ./outputs/20200324-000541/ckpts\\ckpt-12\n",
      "Epoch 60 Loss 2.9212 Accuracy 0.2049\n",
      "Time taken for epoch: 77.32398462295532 secs\n",
      "\n",
      "Epoch 61 Loss 2.9044 Accuracy 0.2115\n",
      "Time taken for epoch: 77.11017084121704 secs\n",
      "\n",
      "Epoch 62 Loss 2.8855 Accuracy 0.2153\n",
      "Time taken for epoch: 76.92346477508545 secs\n",
      "\n",
      "Epoch 63 Loss 2.8857 Accuracy 0.2148\n",
      "Time taken for epoch: 76.65267705917358 secs\n",
      "\n",
      "Epoch 64 Loss 2.8850 Accuracy 0.2157\n",
      "Time taken for epoch: 77.03333711624146 secs\n",
      "\n",
      "Saving checkpoint for epoch 65 at ./outputs/20200324-000541/ckpts\\ckpt-13\n",
      "Epoch 65 Loss 2.8705 Accuracy 0.2170\n",
      "Time taken for epoch: 78.92344689369202 secs\n",
      "\n",
      "Epoch 66 Loss 2.8694 Accuracy 0.2191\n",
      "Time taken for epoch: 77.505122423172 secs\n",
      "\n",
      "Epoch 67 Loss 2.8462 Accuracy 0.2221\n",
      "Time taken for epoch: 76.52715563774109 secs\n",
      "\n",
      "Epoch 68 Loss 2.8542 Accuracy 0.2235\n",
      "Time taken for epoch: 77.04397010803223 secs\n",
      "\n",
      "Epoch 69 Loss 2.8543 Accuracy 0.2220\n",
      "Time taken for epoch: 76.0493540763855 secs\n",
      "\n",
      "Saving checkpoint for epoch 70 at ./outputs/20200324-000541/ckpts\\ckpt-14\n",
      "Epoch 70 Loss 2.8286 Accuracy 0.2363\n",
      "Time taken for epoch: 78.28708696365356 secs\n",
      "\n",
      "Epoch 71 Loss 2.8410 Accuracy 0.2276\n",
      "Time taken for epoch: 77.00338840484619 secs\n",
      "\n",
      "Epoch 72 Loss 2.8274 Accuracy 0.2328\n",
      "Time taken for epoch: 76.35351133346558 secs\n",
      "\n",
      "Epoch 73 Loss 2.8248 Accuracy 0.2309\n",
      "Time taken for epoch: 76.56446647644043 secs\n",
      "\n",
      "Epoch 74 Loss 2.8151 Accuracy 0.2335\n",
      "Time taken for epoch: 76.7661018371582 secs\n",
      "\n",
      "Saving checkpoint for epoch 75 at ./outputs/20200324-000541/ckpts\\ckpt-15\n",
      "Epoch 75 Loss 2.8064 Accuracy 0.2371\n",
      "Time taken for epoch: 78.20721578598022 secs\n",
      "\n",
      "Epoch 76 Loss 2.8111 Accuracy 0.2360\n",
      "Time taken for epoch: 77.19603824615479 secs\n",
      "\n",
      "Epoch 77 Loss 2.7991 Accuracy 0.2345\n",
      "Time taken for epoch: 83.90122413635254 secs\n",
      "\n",
      "Epoch 78 Loss 2.8032 Accuracy 0.2392\n",
      "Time taken for epoch: 77.25651597976685 secs\n",
      "\n",
      "Epoch 79 Loss 2.7740 Accuracy 0.2427\n",
      "Time taken for epoch: 75.05707097053528 secs\n",
      "\n",
      "Saving checkpoint for epoch 80 at ./outputs/20200324-000541/ckpts\\ckpt-16\n",
      "Epoch 80 Loss 2.7729 Accuracy 0.2450\n",
      "Time taken for epoch: 79.26746439933777 secs\n",
      "\n",
      "Epoch 81 Loss 2.7772 Accuracy 0.2405\n",
      "Time taken for epoch: 78.10515141487122 secs\n",
      "\n",
      "Epoch 82 Loss 2.7848 Accuracy 0.2410\n",
      "Time taken for epoch: 77.29860353469849 secs\n",
      "\n",
      "Epoch 83 Loss 2.7657 Accuracy 0.2429\n",
      "Time taken for epoch: 77.1339864730835 secs\n",
      "\n",
      "Epoch 84 Loss 2.7586 Accuracy 0.2474\n",
      "Time taken for epoch: 77.0049946308136 secs\n",
      "\n",
      "Saving checkpoint for epoch 85 at ./outputs/20200324-000541/ckpts\\ckpt-17\n",
      "Epoch 85 Loss 2.7584 Accuracy 0.2442\n",
      "Time taken for epoch: 78.32351231575012 secs\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86 Loss 2.7425 Accuracy 0.2492\n",
      "Time taken for epoch: 77.15404558181763 secs\n",
      "\n",
      "Epoch 87 Loss 2.7536 Accuracy 0.2474\n",
      "Time taken for epoch: 77.68461894989014 secs\n",
      "\n",
      "Epoch 88 Loss 2.7510 Accuracy 0.2485\n",
      "Time taken for epoch: 76.97298860549927 secs\n",
      "\n",
      "Epoch 89 Loss 2.7400 Accuracy 0.2490\n",
      "Time taken for epoch: 77.37986397743225 secs\n",
      "\n",
      "Saving checkpoint for epoch 90 at ./outputs/20200324-000541/ckpts\\ckpt-18\n",
      "Epoch 90 Loss 2.7274 Accuracy 0.2519\n",
      "Time taken for epoch: 77.6064145565033 secs\n",
      "\n",
      "Epoch 91 Loss 2.7366 Accuracy 0.2488\n",
      "Time taken for epoch: 77.46152973175049 secs\n",
      "\n",
      "Epoch 92 Loss 2.7420 Accuracy 0.2505\n",
      "Time taken for epoch: 77.43058395385742 secs\n",
      "\n",
      "Epoch 93 Loss 2.7177 Accuracy 0.2584\n",
      "Time taken for epoch: 77.36686444282532 secs\n",
      "\n",
      "Epoch 94 Loss 2.7270 Accuracy 0.2505\n",
      "Time taken for epoch: 76.23792743682861 secs\n",
      "\n",
      "Saving checkpoint for epoch 95 at ./outputs/20200324-000541/ckpts\\ckpt-19\n",
      "Epoch 95 Loss 2.7175 Accuracy 0.2529\n",
      "Time taken for epoch: 78.28342151641846 secs\n",
      "\n",
      "Epoch 96 Loss 2.7083 Accuracy 0.2551\n",
      "Time taken for epoch: 76.786141872406 secs\n",
      "\n",
      "Epoch 97 Loss 2.7069 Accuracy 0.2566\n",
      "Time taken for epoch: 76.66406893730164 secs\n",
      "\n",
      "Epoch 98 Loss 2.7096 Accuracy 0.2542\n",
      "Time taken for epoch: 76.50119304656982 secs\n",
      "\n",
      "Epoch 99 Loss 2.7077 Accuracy 0.2542\n",
      "Time taken for epoch: 78.29838871955872 secs\n",
      "\n",
      "Saving checkpoint for epoch 100 at ./outputs/20200324-000541/ckpts\\ckpt-20\n",
      "Epoch 100 Loss 2.7024 Accuracy 0.2546\n",
      "Time taken for epoch: 78.07417178153992 secs\n",
      "\n",
      "Epoch 101 Loss 2.6983 Accuracy 0.2587\n",
      "Time taken for epoch: 77.46036100387573 secs\n",
      "\n",
      "Epoch 102 Loss 2.6849 Accuracy 0.2620\n",
      "Time taken for epoch: 76.94041347503662 secs\n",
      "\n",
      "Epoch 103 Loss 2.6996 Accuracy 0.2556\n",
      "Time taken for epoch: 76.89319610595703 secs\n",
      "\n",
      "Epoch 104 Loss 2.6965 Accuracy 0.2579\n",
      "Time taken for epoch: 76.48549151420593 secs\n",
      "\n",
      "Saving checkpoint for epoch 105 at ./outputs/20200324-000541/ckpts\\ckpt-21\n",
      "Epoch 105 Loss 2.6780 Accuracy 0.2591\n",
      "Time taken for epoch: 78.90751791000366 secs\n",
      "\n",
      "Epoch 106 Loss 2.6838 Accuracy 0.2555\n",
      "Time taken for epoch: 77.23415946960449 secs\n",
      "\n",
      "Epoch 107 Loss 2.6781 Accuracy 0.2613\n",
      "Time taken for epoch: 76.81690740585327 secs\n",
      "\n",
      "Epoch 108 Loss 2.6728 Accuracy 0.2619\n",
      "Time taken for epoch: 76.43178248405457 secs\n",
      "\n",
      "Epoch 109 Loss 2.6611 Accuracy 0.2669\n",
      "Time taken for epoch: 76.62012910842896 secs\n",
      "\n",
      "Saving checkpoint for epoch 110 at ./outputs/20200324-000541/ckpts\\ckpt-22\n",
      "Epoch 110 Loss 2.6680 Accuracy 0.2604\n",
      "Time taken for epoch: 78.39840531349182 secs\n",
      "\n",
      "Epoch 111 Loss 2.6520 Accuracy 0.2641\n",
      "Time taken for epoch: 77.21036458015442 secs\n",
      "\n",
      "Epoch 112 Loss 2.6611 Accuracy 0.2641\n",
      "Time taken for epoch: 77.4099326133728 secs\n",
      "\n",
      "Epoch 113 Loss 2.6623 Accuracy 0.2635\n",
      "Time taken for epoch: 77.23046588897705 secs\n",
      "\n",
      "Epoch 114 Loss 2.6423 Accuracy 0.2698\n",
      "Time taken for epoch: 76.88299107551575 secs\n",
      "\n",
      "Saving checkpoint for epoch 115 at ./outputs/20200324-000541/ckpts\\ckpt-23\n",
      "Epoch 115 Loss 2.6527 Accuracy 0.2683\n",
      "Time taken for epoch: 78.19024729728699 secs\n",
      "\n",
      "Epoch 116 Loss 2.6464 Accuracy 0.2656\n",
      "Time taken for epoch: 77.02752566337585 secs\n",
      "\n",
      "Epoch 117 Loss 2.6381 Accuracy 0.2663\n",
      "Time taken for epoch: 77.35816860198975 secs\n",
      "\n",
      "Epoch 118 Loss 2.6243 Accuracy 0.2714\n",
      "Time taken for epoch: 76.93606781959534 secs\n",
      "\n",
      "Epoch 119 Loss 2.6299 Accuracy 0.2701\n",
      "Time taken for epoch: 76.66063046455383 secs\n",
      "\n",
      "Saving checkpoint for epoch 120 at ./outputs/20200324-000541/ckpts\\ckpt-24\n",
      "Epoch 120 Loss 2.6363 Accuracy 0.2710\n",
      "Time taken for epoch: 78.06381273269653 secs\n",
      "\n",
      "Epoch 121 Loss 2.6317 Accuracy 0.2680\n",
      "Time taken for epoch: 77.29148769378662 secs\n",
      "\n",
      "Epoch 122 Loss 2.6227 Accuracy 0.2711\n",
      "Time taken for epoch: 77.07971143722534 secs\n",
      "\n",
      "Epoch 123 Loss 2.6207 Accuracy 0.2698\n",
      "Time taken for epoch: 77.34320378303528 secs\n",
      "\n",
      "Epoch 124 Loss 2.6335 Accuracy 0.2690\n",
      "Time taken for epoch: 76.97708821296692 secs\n",
      "\n",
      "Saving checkpoint for epoch 125 at ./outputs/20200324-000541/ckpts\\ckpt-25\n",
      "Epoch 125 Loss 2.6217 Accuracy 0.2710\n",
      "Time taken for epoch: 78.40966320037842 secs\n",
      "\n",
      "Epoch 126 Loss 2.6201 Accuracy 0.2715\n",
      "Time taken for epoch: 76.98572301864624 secs\n",
      "\n",
      "Epoch 127 Loss 2.6321 Accuracy 0.2679\n",
      "Time taken for epoch: 76.86873579025269 secs\n",
      "\n",
      "Epoch 128 Loss 2.6094 Accuracy 0.2725\n",
      "Time taken for epoch: 76.42570233345032 secs\n",
      "\n",
      "Epoch 129 Loss 2.6166 Accuracy 0.2730\n",
      "Time taken for epoch: 76.63004732131958 secs\n",
      "\n",
      "Saving checkpoint for epoch 130 at ./outputs/20200324-000541/ckpts\\ckpt-26\n",
      "Epoch 130 Loss 2.6092 Accuracy 0.2744\n",
      "Time taken for epoch: 78.01607155799866 secs\n",
      "\n",
      "Epoch 131 Loss 2.6054 Accuracy 0.2714\n",
      "Time taken for epoch: 76.97985291481018 secs\n",
      "\n",
      "Epoch 132 Loss 2.6031 Accuracy 0.2717\n",
      "Time taken for epoch: 77.18182253837585 secs\n",
      "\n",
      "Epoch 133 Loss 2.6021 Accuracy 0.2740\n",
      "Time taken for epoch: 77.08205986022949 secs\n",
      "\n",
      "Epoch 134 Loss 2.6010 Accuracy 0.2757\n",
      "Time taken for epoch: 76.40818357467651 secs\n",
      "\n",
      "Saving checkpoint for epoch 135 at ./outputs/20200324-000541/ckpts\\ckpt-27\n",
      "Epoch 135 Loss 2.5884 Accuracy 0.2753\n",
      "Time taken for epoch: 78.46922302246094 secs\n",
      "\n",
      "Epoch 136 Loss 2.6060 Accuracy 0.2728\n",
      "Time taken for epoch: 76.97997260093689 secs\n",
      "\n",
      "Epoch 137 Loss 2.5883 Accuracy 0.2733\n",
      "Time taken for epoch: 76.83258700370789 secs\n",
      "\n",
      "Epoch 138 Loss 2.5916 Accuracy 0.2748\n",
      "Time taken for epoch: 77.4811041355133 secs\n",
      "\n",
      "Epoch 139 Loss 2.5864 Accuracy 0.2710\n",
      "Time taken for epoch: 76.47367000579834 secs\n",
      "\n",
      "Saving checkpoint for epoch 140 at ./outputs/20200324-000541/ckpts\\ckpt-28\n",
      "Epoch 140 Loss 2.5914 Accuracy 0.2788\n",
      "Time taken for epoch: 78.11559677124023 secs\n",
      "\n",
      "Epoch 141 Loss 2.5781 Accuracy 0.2786\n",
      "Time taken for epoch: 77.50872850418091 secs\n",
      "\n",
      "Epoch 142 Loss 2.5819 Accuracy 0.2778\n",
      "Time taken for epoch: 77.12333726882935 secs\n",
      "\n",
      "Epoch 143 Loss 2.5753 Accuracy 0.2759\n",
      "Time taken for epoch: 77.09410810470581 secs\n",
      "\n",
      "Epoch 144 Loss 2.5857 Accuracy 0.2707\n",
      "Time taken for epoch: 76.97635293006897 secs\n",
      "\n",
      "Saving checkpoint for epoch 145 at ./outputs/20200324-000541/ckpts\\ckpt-29\n",
      "Epoch 145 Loss 2.5683 Accuracy 0.2763\n",
      "Time taken for epoch: 78.20971894264221 secs\n",
      "\n",
      "Epoch 146 Loss 2.5811 Accuracy 0.2729\n",
      "Time taken for epoch: 77.39193439483643 secs\n",
      "\n",
      "Epoch 147 Loss 2.5650 Accuracy 0.2802\n",
      "Time taken for epoch: 77.39775228500366 secs\n",
      "\n",
      "Epoch 148 Loss 2.5645 Accuracy 0.2775\n",
      "Time taken for epoch: 77.1124792098999 secs\n",
      "\n",
      "Epoch 149 Loss 2.5704 Accuracy 0.2758\n",
      "Time taken for epoch: 77.54560828208923 secs\n",
      "\n",
      "Saving checkpoint for epoch 150 at ./outputs/20200324-000541/ckpts\\ckpt-30\n",
      "Epoch 150 Loss 2.5600 Accuracy 0.2794\n",
      "Time taken for epoch: 78.34987545013428 secs\n",
      "\n",
      "Epoch 151 Loss 2.5580 Accuracy 0.2815\n",
      "Time taken for epoch: 77.45738887786865 secs\n",
      "\n",
      "Epoch 152 Loss 2.5600 Accuracy 0.2813\n",
      "Time taken for epoch: 77.17236638069153 secs\n",
      "\n",
      "Epoch 153 Loss 2.5557 Accuracy 0.2818\n",
      "Time taken for epoch: 76.42689657211304 secs\n",
      "\n",
      "Epoch 154 Loss 2.5516 Accuracy 0.2813\n",
      "Time taken for epoch: 76.75746273994446 secs\n",
      "\n",
      "Saving checkpoint for epoch 155 at ./outputs/20200324-000541/ckpts\\ckpt-31\n",
      "Epoch 155 Loss 2.5547 Accuracy 0.2822\n",
      "Time taken for epoch: 78.02638506889343 secs\n",
      "\n",
      "Epoch 156 Loss 2.5589 Accuracy 0.2788\n",
      "Time taken for epoch: 77.35333442687988 secs\n",
      "\n",
      "Epoch 157 Loss 2.5541 Accuracy 0.2840\n",
      "Time taken for epoch: 77.14665818214417 secs\n",
      "\n",
      "Epoch 158 Loss 2.5534 Accuracy 0.2809\n",
      "Time taken for epoch: 77.34964203834534 secs\n",
      "\n",
      "Epoch 159 Loss 2.5439 Accuracy 0.2798\n",
      "Time taken for epoch: 77.13379621505737 secs\n",
      "\n",
      "Saving checkpoint for epoch 160 at ./outputs/20200324-000541/ckpts\\ckpt-32\n",
      "Epoch 160 Loss 2.5370 Accuracy 0.2864\n",
      "Time taken for epoch: 78.225821018219 secs\n",
      "\n",
      "Epoch 161 Loss 2.5390 Accuracy 0.2831\n",
      "Time taken for epoch: 77.78512501716614 secs\n",
      "\n",
      "Epoch 162 Loss 2.5467 Accuracy 0.2818\n",
      "Time taken for epoch: 76.7367353439331 secs\n",
      "\n",
      "Epoch 163 Loss 2.5423 Accuracy 0.2853\n",
      "Time taken for epoch: 77.34765458106995 secs\n",
      "\n",
      "Epoch 164 Loss 2.5286 Accuracy 0.2828\n",
      "Time taken for epoch: 76.39911937713623 secs\n",
      "\n",
      "Saving checkpoint for epoch 165 at ./outputs/20200324-000541/ckpts\\ckpt-33\n",
      "Epoch 165 Loss 2.5325 Accuracy 0.2862\n",
      "Time taken for epoch: 78.55034613609314 secs\n",
      "\n",
      "Epoch 166 Loss 2.5370 Accuracy 0.2827\n",
      "Time taken for epoch: 76.73274040222168 secs\n",
      "\n",
      "Epoch 167 Loss 2.5325 Accuracy 0.2842\n",
      "Time taken for epoch: 76.51579260826111 secs\n",
      "\n",
      "Epoch 168 Loss 2.5215 Accuracy 0.2858\n",
      "Time taken for epoch: 76.64847421646118 secs\n",
      "\n",
      "Epoch 169 Loss 2.5208 Accuracy 0.2872\n",
      "Time taken for epoch: 76.56386995315552 secs\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint for epoch 170 at ./outputs/20200324-000541/ckpts\\ckpt-34\n",
      "Epoch 170 Loss 2.5322 Accuracy 0.2833\n",
      "Time taken for epoch: 78.34957790374756 secs\n",
      "\n",
      "Epoch 171 Loss 2.5348 Accuracy 0.2854\n",
      "Time taken for epoch: 76.8781852722168 secs\n",
      "\n",
      "Epoch 172 Loss 2.5230 Accuracy 0.2849\n",
      "Time taken for epoch: 77.1845772266388 secs\n",
      "\n",
      "Epoch 173 Loss 2.5337 Accuracy 0.2825\n",
      "Time taken for epoch: 76.46005392074585 secs\n",
      "\n",
      "Epoch 174 Loss 2.5228 Accuracy 0.2869\n",
      "Time taken for epoch: 76.54324460029602 secs\n",
      "\n",
      "Saving checkpoint for epoch 175 at ./outputs/20200324-000541/ckpts\\ckpt-35\n",
      "Epoch 175 Loss 2.5151 Accuracy 0.2891\n",
      "Time taken for epoch: 78.48339939117432 secs\n",
      "\n",
      "Epoch 176 Loss 2.5101 Accuracy 0.2909\n",
      "Time taken for epoch: 76.94967222213745 secs\n",
      "\n",
      "Epoch 177 Loss 2.5094 Accuracy 0.2893\n",
      "Time taken for epoch: 77.11376357078552 secs\n",
      "\n",
      "Epoch 178 Loss 2.5101 Accuracy 0.2897\n",
      "Time taken for epoch: 77.26440978050232 secs\n",
      "\n",
      "Epoch 179 Loss 2.5132 Accuracy 0.2902\n",
      "Time taken for epoch: 77.05408310890198 secs\n",
      "\n",
      "Saving checkpoint for epoch 180 at ./outputs/20200324-000541/ckpts\\ckpt-36\n",
      "Epoch 180 Loss 2.5002 Accuracy 0.2912\n",
      "Time taken for epoch: 78.68991160392761 secs\n",
      "\n",
      "Epoch 181 Loss 2.5132 Accuracy 0.2903\n",
      "Time taken for epoch: 77.23344445228577 secs\n",
      "\n",
      "Epoch 182 Loss 2.5040 Accuracy 0.2857\n",
      "Time taken for epoch: 77.21044182777405 secs\n",
      "\n",
      "Epoch 183 Loss 2.5029 Accuracy 0.2898\n",
      "Time taken for epoch: 76.94639229774475 secs\n",
      "\n",
      "Epoch 184 Loss 2.5055 Accuracy 0.2903\n",
      "Time taken for epoch: 77.33599257469177 secs\n",
      "\n",
      "Saving checkpoint for epoch 185 at ./outputs/20200324-000541/ckpts\\ckpt-37\n",
      "Epoch 185 Loss 2.5074 Accuracy 0.2881\n",
      "Time taken for epoch: 78.54506397247314 secs\n",
      "\n",
      "Epoch 186 Loss 2.5022 Accuracy 0.2904\n",
      "Time taken for epoch: 77.51496028900146 secs\n",
      "\n",
      "Epoch 187 Loss 2.4846 Accuracy 0.2974\n",
      "Time taken for epoch: 77.49032974243164 secs\n",
      "\n",
      "Epoch 188 Loss 2.4945 Accuracy 0.2938\n",
      "Time taken for epoch: 77.33476328849792 secs\n",
      "\n",
      "Epoch 189 Loss 2.4920 Accuracy 0.2946\n",
      "Time taken for epoch: 76.73587679862976 secs\n",
      "\n",
      "Saving checkpoint for epoch 190 at ./outputs/20200324-000541/ckpts\\ckpt-38\n",
      "Epoch 190 Loss 2.4892 Accuracy 0.2933\n",
      "Time taken for epoch: 78.52216124534607 secs\n",
      "\n",
      "Epoch 191 Loss 2.5016 Accuracy 0.2929\n",
      "Time taken for epoch: 77.72787594795227 secs\n",
      "\n",
      "Epoch 192 Loss 2.4860 Accuracy 0.2935\n",
      "Time taken for epoch: 77.01624536514282 secs\n",
      "\n",
      "Epoch 193 Loss 2.4754 Accuracy 0.2961\n",
      "Time taken for epoch: 77.65286588668823 secs\n",
      "\n",
      "Epoch 194 Loss 2.4948 Accuracy 0.2927\n",
      "Time taken for epoch: 76.6738486289978 secs\n",
      "\n",
      "Saving checkpoint for epoch 195 at ./outputs/20200324-000541/ckpts\\ckpt-39\n",
      "Epoch 195 Loss 2.4793 Accuracy 0.2976\n",
      "Time taken for epoch: 78.45332026481628 secs\n",
      "\n",
      "Epoch 196 Loss 2.4792 Accuracy 0.2974\n",
      "Time taken for epoch: 77.22062802314758 secs\n",
      "\n",
      "Epoch 197 Loss 2.4862 Accuracy 0.2938\n",
      "Time taken for epoch: 77.2298276424408 secs\n",
      "\n",
      "Epoch 198 Loss 2.4850 Accuracy 0.2937\n",
      "Time taken for epoch: 76.8951268196106 secs\n",
      "\n",
      "Epoch 199 Loss 2.4839 Accuracy 0.2959\n",
      "Time taken for epoch: 76.65211820602417 secs\n",
      "\n",
      "Saving checkpoint for epoch 200 at ./outputs/20200324-000541/ckpts\\ckpt-40\n",
      "Epoch 200 Loss 2.4967 Accuracy 0.2934\n",
      "Time taken for epoch: 78.43502807617188 secs\n",
      "\n",
      "Epoch 201 Loss 2.4743 Accuracy 0.2971\n",
      "Time taken for epoch: 78.14361453056335 secs\n",
      "\n",
      "Epoch 202 Loss 2.4876 Accuracy 0.2950\n",
      "Time taken for epoch: 76.27593421936035 secs\n",
      "\n",
      "Epoch 203 Loss 2.4732 Accuracy 0.2984\n",
      "Time taken for epoch: 76.62552571296692 secs\n",
      "\n",
      "Epoch 204 Loss 2.4646 Accuracy 0.2977\n",
      "Time taken for epoch: 76.61915302276611 secs\n",
      "\n",
      "Saving checkpoint for epoch 205 at ./outputs/20200324-000541/ckpts\\ckpt-41\n",
      "Epoch 205 Loss 2.4807 Accuracy 0.2930\n",
      "Time taken for epoch: 78.13160848617554 secs\n",
      "\n",
      "Epoch 206 Loss 2.4710 Accuracy 0.2966\n",
      "Time taken for epoch: 77.57329535484314 secs\n",
      "\n",
      "Epoch 207 Loss 2.4648 Accuracy 0.2995\n",
      "Time taken for epoch: 76.98839092254639 secs\n",
      "\n",
      "Epoch 208 Loss 2.4696 Accuracy 0.2970\n",
      "Time taken for epoch: 78.36971020698547 secs\n",
      "\n",
      "Epoch 209 Loss 2.4646 Accuracy 0.3020\n",
      "Time taken for epoch: 80.81561470031738 secs\n",
      "\n",
      "Saving checkpoint for epoch 210 at ./outputs/20200324-000541/ckpts\\ckpt-42\n",
      "Epoch 210 Loss 2.4710 Accuracy 0.2967\n",
      "Time taken for epoch: 119.26302933692932 secs\n",
      "\n",
      "Epoch 211 Loss 2.4577 Accuracy 0.2992\n",
      "Time taken for epoch: 112.1346082687378 secs\n",
      "\n",
      "Epoch 212 Loss 2.4635 Accuracy 0.2983\n",
      "Time taken for epoch: 161.82463002204895 secs\n",
      "\n",
      "Epoch 213 Loss 2.4604 Accuracy 0.2971\n",
      "Time taken for epoch: 141.13363671302795 secs\n",
      "\n",
      "Epoch 214 Loss 2.4519 Accuracy 0.3033\n",
      "Time taken for epoch: 178.4883861541748 secs\n",
      "\n",
      "Saving checkpoint for epoch 215 at ./outputs/20200324-000541/ckpts\\ckpt-43\n",
      "Epoch 215 Loss 2.4588 Accuracy 0.3001\n",
      "Time taken for epoch: 135.09206700325012 secs\n",
      "\n",
      "Epoch 216 Loss 2.4600 Accuracy 0.2983\n",
      "Time taken for epoch: 176.62080574035645 secs\n",
      "\n",
      "Epoch 217 Loss 2.4504 Accuracy 0.3038\n",
      "Time taken for epoch: 164.66973614692688 secs\n",
      "\n",
      "Epoch 218 Loss 2.4594 Accuracy 0.3002\n",
      "Time taken for epoch: 147.05985713005066 secs\n",
      "\n",
      "Epoch 219 Loss 2.4566 Accuracy 0.2998\n",
      "Time taken for epoch: 179.59547591209412 secs\n",
      "\n",
      "Saving checkpoint for epoch 220 at ./outputs/20200324-000541/ckpts\\ckpt-44\n",
      "Epoch 220 Loss 2.4506 Accuracy 0.3029\n",
      "Time taken for epoch: 169.86578631401062 secs\n",
      "\n",
      "Epoch 221 Loss 2.4554 Accuracy 0.2988\n",
      "Time taken for epoch: 157.3070785999298 secs\n",
      "\n",
      "Epoch 222 Loss 2.4550 Accuracy 0.3015\n",
      "Time taken for epoch: 180.75418877601624 secs\n",
      "\n",
      "Epoch 223 Loss 2.4483 Accuracy 0.3018\n",
      "Time taken for epoch: 163.21307063102722 secs\n",
      "\n",
      "Epoch 224 Loss 2.4531 Accuracy 0.3035\n",
      "Time taken for epoch: 159.49184107780457 secs\n",
      "\n",
      "Saving checkpoint for epoch 225 at ./outputs/20200324-000541/ckpts\\ckpt-45\n",
      "Epoch 225 Loss 2.4468 Accuracy 0.3052\n",
      "Time taken for epoch: 174.0473325252533 secs\n",
      "\n",
      "Epoch 226 Loss 2.4521 Accuracy 0.3021\n",
      "Time taken for epoch: 142.28187465667725 secs\n",
      "\n",
      "Epoch 227 Loss 2.4403 Accuracy 0.3041\n",
      "Time taken for epoch: 183.4165301322937 secs\n",
      "\n",
      "Epoch 228 Loss 2.4548 Accuracy 0.3019\n",
      "Time taken for epoch: 142.8134367465973 secs\n",
      "\n",
      "Epoch 229 Loss 2.4579 Accuracy 0.3031\n",
      "Time taken for epoch: 151.00340819358826 secs\n",
      "\n",
      "Saving checkpoint for epoch 230 at ./outputs/20200324-000541/ckpts\\ckpt-46\n",
      "Epoch 230 Loss 2.4245 Accuracy 0.3058\n",
      "Time taken for epoch: 146.012943983078 secs\n",
      "\n",
      "Epoch 231 Loss 2.4462 Accuracy 0.3034\n",
      "Time taken for epoch: 147.93618726730347 secs\n",
      "\n",
      "Epoch 232 Loss 2.4438 Accuracy 0.3032\n",
      "Time taken for epoch: 151.97762179374695 secs\n",
      "\n",
      "Epoch 233 Loss 2.4395 Accuracy 0.3003\n",
      "Time taken for epoch: 151.94789218902588 secs\n",
      "\n",
      "Epoch 234 Loss 2.4421 Accuracy 0.3060\n",
      "Time taken for epoch: 151.02278089523315 secs\n",
      "\n",
      "Saving checkpoint for epoch 235 at ./outputs/20200324-000541/ckpts\\ckpt-47\n",
      "Epoch 235 Loss 2.4313 Accuracy 0.3059\n",
      "Time taken for epoch: 153.04940557479858 secs\n",
      "\n",
      "Epoch 236 Loss 2.4284 Accuracy 0.3095\n",
      "Time taken for epoch: 152.81389594078064 secs\n",
      "\n",
      "Epoch 237 Loss 2.4411 Accuracy 0.3039\n",
      "Time taken for epoch: 152.55155539512634 secs\n",
      "\n",
      "Epoch 238 Loss 2.4273 Accuracy 0.3097\n",
      "Time taken for epoch: 140.6598038673401 secs\n",
      "\n",
      "Epoch 239 Loss 2.4243 Accuracy 0.3081\n",
      "Time taken for epoch: 113.2007429599762 secs\n",
      "\n",
      "Saving checkpoint for epoch 240 at ./outputs/20200324-000541/ckpts\\ckpt-48\n",
      "Epoch 240 Loss 2.4363 Accuracy 0.3094\n",
      "Time taken for epoch: 162.1504213809967 secs\n",
      "\n",
      "Epoch 241 Loss 2.4327 Accuracy 0.3056\n",
      "Time taken for epoch: 135.92972922325134 secs\n",
      "\n",
      "Epoch 242 Loss 2.4277 Accuracy 0.3062\n",
      "Time taken for epoch: 140.93673181533813 secs\n",
      "\n",
      "Epoch 243 Loss 2.4235 Accuracy 0.3112\n",
      "Time taken for epoch: 158.6458864212036 secs\n",
      "\n",
      "Epoch 244 Loss 2.4298 Accuracy 0.3070\n",
      "Time taken for epoch: 141.47218084335327 secs\n",
      "\n",
      "Saving checkpoint for epoch 245 at ./outputs/20200324-000541/ckpts\\ckpt-49\n",
      "Epoch 245 Loss 2.4298 Accuracy 0.3079\n",
      "Time taken for epoch: 143.00029301643372 secs\n",
      "\n",
      "Epoch 246 Loss 2.4293 Accuracy 0.3048\n",
      "Time taken for epoch: 164.15592217445374 secs\n",
      "\n",
      "Epoch 247 Loss 2.4293 Accuracy 0.3057\n",
      "Time taken for epoch: 141.6957004070282 secs\n",
      "\n",
      "Epoch 248 Loss 2.4130 Accuracy 0.3122\n",
      "Time taken for epoch: 143.1496000289917 secs\n",
      "\n",
      "Epoch 249 Loss 2.4102 Accuracy 0.3137\n",
      "Time taken for epoch: 157.75301575660706 secs\n",
      "\n",
      "Saving checkpoint for epoch 250 at ./outputs/20200324-000541/ckpts\\ckpt-50\n",
      "Epoch 250 Loss 2.4152 Accuracy 0.3123\n",
      "Time taken for epoch: 142.46161007881165 secs\n",
      "\n",
      "Epoch 251 Loss 2.4204 Accuracy 0.3086\n",
      "Time taken for epoch: 150.45159244537354 secs\n",
      "\n",
      "Epoch 252 Loss 2.4129 Accuracy 0.3103\n",
      "Time taken for epoch: 154.46910095214844 secs\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 253 Loss 2.4165 Accuracy 0.3111\n",
      "Time taken for epoch: 140.8692545890808 secs\n",
      "\n",
      "Epoch 254 Loss 2.4248 Accuracy 0.3065\n",
      "Time taken for epoch: 146.73821353912354 secs\n",
      "\n",
      "Saving checkpoint for epoch 255 at ./outputs/20200324-000541/ckpts\\ckpt-51\n",
      "Epoch 255 Loss 2.4075 Accuracy 0.3144\n",
      "Time taken for epoch: 158.61984992027283 secs\n",
      "\n",
      "Epoch 256 Loss 2.4167 Accuracy 0.3084\n",
      "Time taken for epoch: 128.0950837135315 secs\n",
      "\n",
      "Epoch 257 Loss 2.3959 Accuracy 0.3148\n",
      "Time taken for epoch: 155.25612950325012 secs\n",
      "\n",
      "Epoch 258 Loss 2.4140 Accuracy 0.3108\n",
      "Time taken for epoch: 152.9687249660492 secs\n",
      "\n",
      "Epoch 259 Loss 2.3989 Accuracy 0.3113\n",
      "Time taken for epoch: 152.3165202140808 secs\n",
      "\n",
      "Saving checkpoint for epoch 260 at ./outputs/20200324-000541/ckpts\\ckpt-52\n",
      "Epoch 260 Loss 2.4023 Accuracy 0.3157\n",
      "Time taken for epoch: 153.6019208431244 secs\n",
      "\n",
      "Epoch 261 Loss 2.3999 Accuracy 0.3185\n",
      "Time taken for epoch: 152.88019275665283 secs\n",
      "\n",
      "Epoch 262 Loss 2.3955 Accuracy 0.3128\n",
      "Time taken for epoch: 151.47720575332642 secs\n",
      "\n",
      "Epoch 263 Loss 2.4073 Accuracy 0.3144\n",
      "Time taken for epoch: 151.0638074874878 secs\n",
      "\n",
      "Epoch 264 Loss 2.3910 Accuracy 0.3138\n",
      "Time taken for epoch: 140.49948000907898 secs\n",
      "\n",
      "Saving checkpoint for epoch 265 at ./outputs/20200324-000541/ckpts\\ckpt-53\n",
      "Epoch 265 Loss 2.3944 Accuracy 0.3173\n",
      "Time taken for epoch: 119.85716462135315 secs\n",
      "\n",
      "Epoch 266 Loss 2.3930 Accuracy 0.3182\n",
      "Time taken for epoch: 165.85248112678528 secs\n",
      "\n",
      "Epoch 267 Loss 2.3939 Accuracy 0.3160\n",
      "Time taken for epoch: 140.68249654769897 secs\n",
      "\n",
      "Epoch 268 Loss 2.3951 Accuracy 0.3171\n",
      "Time taken for epoch: 148.3516149520874 secs\n",
      "\n",
      "Epoch 269 Loss 2.3923 Accuracy 0.3148\n",
      "Time taken for epoch: 154.65749669075012 secs\n",
      "\n",
      "Saving checkpoint for epoch 270 at ./outputs/20200324-000541/ckpts\\ckpt-54\n",
      "Epoch 270 Loss 2.3926 Accuracy 0.3179\n",
      "Time taken for epoch: 142.2200107574463 secs\n",
      "\n",
      "Epoch 271 Loss 2.3917 Accuracy 0.3139\n",
      "Time taken for epoch: 156.93093037605286 secs\n",
      "\n",
      "Epoch 272 Loss 2.3825 Accuracy 0.3185\n",
      "Time taken for epoch: 150.34502506256104 secs\n",
      "\n",
      "Epoch 273 Loss 2.3926 Accuracy 0.3148\n",
      "Time taken for epoch: 140.8582739830017 secs\n",
      "\n",
      "Epoch 274 Loss 2.3875 Accuracy 0.3171\n",
      "Time taken for epoch: 156.31713891029358 secs\n",
      "\n",
      "Saving checkpoint for epoch 275 at ./outputs/20200324-000541/ckpts\\ckpt-55\n",
      "Epoch 275 Loss 2.3889 Accuracy 0.3152\n",
      "Time taken for epoch: 155.07417726516724 secs\n",
      "\n",
      "Epoch 276 Loss 2.3799 Accuracy 0.3209\n",
      "Time taken for epoch: 127.28358387947083 secs\n",
      "\n",
      "Epoch 277 Loss 2.3754 Accuracy 0.3194\n",
      "Time taken for epoch: 156.23135924339294 secs\n",
      "\n",
      "Epoch 278 Loss 2.3880 Accuracy 0.3183\n",
      "Time taken for epoch: 140.51072025299072 secs\n",
      "\n",
      "Epoch 279 Loss 2.3896 Accuracy 0.3181\n",
      "Time taken for epoch: 140.6437633037567 secs\n",
      "\n",
      "Saving checkpoint for epoch 280 at ./outputs/20200324-000541/ckpts\\ckpt-56\n",
      "Epoch 280 Loss 2.3776 Accuracy 0.3205\n",
      "Time taken for epoch: 170.2982041835785 secs\n",
      "\n",
      "Epoch 281 Loss 2.3887 Accuracy 0.3193\n",
      "Time taken for epoch: 141.91500210762024 secs\n",
      "\n",
      "Epoch 282 Loss 2.3761 Accuracy 0.3198\n",
      "Time taken for epoch: 116.35615634918213 secs\n",
      "\n",
      "Epoch 283 Loss 2.3715 Accuracy 0.3233\n",
      "Time taken for epoch: 156.86873722076416 secs\n",
      "\n",
      "Epoch 284 Loss 2.3795 Accuracy 0.3161\n",
      "Time taken for epoch: 150.66918301582336 secs\n",
      "\n",
      "Saving checkpoint for epoch 285 at ./outputs/20200324-000541/ckpts\\ckpt-57\n",
      "Epoch 285 Loss 2.3682 Accuracy 0.3224\n",
      "Time taken for epoch: 153.63188433647156 secs\n",
      "\n",
      "Epoch 286 Loss 2.3691 Accuracy 0.3177\n",
      "Time taken for epoch: 153.53375577926636 secs\n",
      "\n",
      "Epoch 287 Loss 2.3788 Accuracy 0.3188\n",
      "Time taken for epoch: 152.22135734558105 secs\n",
      "\n",
      "Epoch 288 Loss 2.3656 Accuracy 0.3237\n",
      "Time taken for epoch: 151.43392181396484 secs\n",
      "\n",
      "Epoch 289 Loss 2.3693 Accuracy 0.3218\n",
      "Time taken for epoch: 151.57483315467834 secs\n",
      "\n",
      "Saving checkpoint for epoch 290 at ./outputs/20200324-000541/ckpts\\ckpt-58\n",
      "Epoch 290 Loss 2.3699 Accuracy 0.3242\n",
      "Time taken for epoch: 153.79765605926514 secs\n",
      "\n",
      "Epoch 291 Loss 2.3634 Accuracy 0.3254\n",
      "Time taken for epoch: 152.8034462928772 secs\n",
      "\n",
      "Epoch 292 Loss 2.3672 Accuracy 0.3249\n",
      "Time taken for epoch: 152.18640637397766 secs\n",
      "\n",
      "Epoch 293 Loss 2.3770 Accuracy 0.3189\n",
      "Time taken for epoch: 151.20578575134277 secs\n",
      "\n",
      "Epoch 294 Loss 2.3752 Accuracy 0.3175\n",
      "Time taken for epoch: 151.6449854373932 secs\n",
      "\n",
      "Saving checkpoint for epoch 295 at ./outputs/20200324-000541/ckpts\\ckpt-59\n",
      "Epoch 295 Loss 2.3562 Accuracy 0.3254\n",
      "Time taken for epoch: 154.0921666622162 secs\n",
      "\n",
      "Epoch 296 Loss 2.3741 Accuracy 0.3224\n",
      "Time taken for epoch: 152.54815030097961 secs\n",
      "\n",
      "Epoch 297 Loss 2.3577 Accuracy 0.3213\n",
      "Time taken for epoch: 150.19714879989624 secs\n",
      "\n",
      "Epoch 298 Loss 2.3600 Accuracy 0.3217\n",
      "Time taken for epoch: 151.32958722114563 secs\n",
      "\n",
      "Epoch 299 Loss 2.3686 Accuracy 0.3219\n",
      "Time taken for epoch: 151.32160425186157 secs\n",
      "\n",
      "Saving checkpoint for epoch 300 at ./outputs/20200324-000541/ckpts\\ckpt-60\n",
      "Epoch 300 Loss 2.3440 Accuracy 0.3272\n",
      "Time taken for epoch: 153.80498027801514 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    while epoch < EPOCHS:\n",
    "        start = time.time()\n",
    "\n",
    "        train_loss.reset_states()\n",
    "        train_acc.reset_states()\n",
    "\n",
    "        for batch, tar in enumerate(train_dataset):\n",
    "            train_step_tf(tar)\n",
    "\n",
    "            if batch % 50 == 0:\n",
    "#                 print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "#                     epoch + 1, batch, train_loss.result(), train_acc.result()))\n",
    "\n",
    "                with summary_writer.as_default():\n",
    "                    tf.summary.scalar('loss', train_loss.result(), \n",
    "                                      step=epoch)\n",
    "                    tf.summary.scalar('accuracy', train_acc.result(), \n",
    "                                      step=epoch)\n",
    "\n",
    "            if batch >= BATCHES_IN_EPOCH:\n",
    "                break\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            ckpt_save_path = ckpt_manager.save()\n",
    "            print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                                 ckpt_save_path))\n",
    "\n",
    "        print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
    "                                                             train_loss.result(), \n",
    "                                                             train_acc.result()))\n",
    "\n",
    "        print ('Time taken for epoch: {} secs\\n'.format(time.time() - start))\n",
    "        epoch += 1\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print('Manual interrupt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint restored!!\n"
     ]
    }
   ],
   "source": [
    "ckpt_loc = 'outputs/20200321-171346/ckpts'\n",
    "ckpt = tf.train.Checkpoint(transformer=model,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, ckpt_loc, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'outputs/20200321-171346/ckpts\\\\ckpt-60'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_manager.latest_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=6.7651315>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tar = next(iter(train_dataset))\n",
    "inp, where_masked = create_mask_and_input(tar)\n",
    "attention_mask = tf.cast(inp != pad_token, tf.int32)\n",
    "logits, *_ = model(inp, attention_mask=attention_mask)\n",
    "loss = loss_fn(tar[where_masked], logits[where_masked])\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_string(inp[0]), to_string(tar[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_string(tf.argmax(logits[0][inp[0] == mask_token], 1)), to_string(tar[0][inp[0] == mask_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def generate_text(\n",
    "    model: tf.keras.Model,\n",
    "    start: str,\n",
    "    mask_token: int,\n",
    "    temperature = 1.0,\n",
    "    steps=20,\n",
    "    print_process=True\n",
    "):\n",
    "    if print_process:\n",
    "        print(start)\n",
    "        print('------')\n",
    "    \n",
    "    inp = [char2idx[c] for c in start]\n",
    "    \n",
    "    for _ in range(steps):\n",
    "        inp_with_mask_at_end = tf.concat([inp, [mask_token]], 0)\n",
    "        inp_with_mask_at_end = tf.expand_dims(inp_with_mask_at_end, 0)\n",
    "        outputs = model(inp_with_mask_at_end)\n",
    "        \n",
    "        next_chr_preds = outputs[0][0][-1]\n",
    "        next_chr_preds = next_chr_preds / temperature\n",
    "        next_chr_preds = tf.expand_dims(next_chr_preds, 0)\n",
    "        predicted_id = tf.random.categorical(next_chr_preds, \n",
    "                                             num_samples=1,\n",
    "                                             dtype=tf.int32)[-1, 0]\n",
    "\n",
    "        inp = tf.concat([inp, tf.reshape(predicted_id, (1,))], 0)\n",
    "        \n",
    "        if print_process:\n",
    "            print(to_string(inp))\n",
    "            print('------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text(model, 'ROMEO:', mask_token, print_process=True, temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_weights(attention, sentence, layer):\n",
    "    fig = plt.figure(figsize=(16, 2))\n",
    "\n",
    "    attention = tf.squeeze(attention[layer], axis=0)\n",
    "\n",
    "    for head in range(attention.shape[0]):\n",
    "        ax = fig.add_subplot(2, 4, head+1)\n",
    "\n",
    "        # plot the attention weights\n",
    "        ax.matshow(attention[head], \n",
    "                   cmap='viridis')\n",
    "\n",
    "        fontdict = {'fontsize': 10}\n",
    "\n",
    "        ax.set_xticks(range(len(sentence)+2))\n",
    "\n",
    "        ax.set_xticklabels(\n",
    "            ['-']+[idx2char[i] for i in sentence]+['-'], \n",
    "            fontdict=fontdict)\n",
    "\n",
    "        ax.set_yticklabels([])\n",
    "\n",
    "        ax.set_xlabel('Head {}'.format(head+1))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_attention_weights(attention_weights, x.numpy(), 'decoder_layer1_block2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
