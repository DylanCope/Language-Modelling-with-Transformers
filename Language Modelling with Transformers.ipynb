{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physical Devices:\n",
      " [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')] \n",
      "\n",
      "\n",
      "Output Directory: ./outputs/20200320-201546\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.extend(['..'])\n",
    "\n",
    "from datetime import datetime \n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "import random\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "print('Physical Devices:\\n', tf.config.list_physical_devices(), '\\n')\n",
    "%load_ext tensorboard\n",
    "\n",
    "import transformers\n",
    "\n",
    "\n",
    "stamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "OUTPUTS_DIR = f'./outputs/{stamp}'\n",
    "print('\\nOutput Directory:', OUTPUTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./outputs/20200320-201546/logs\n"
     ]
    }
   ],
   "source": [
    "log_dir = f'{OUTPUTS_DIR}/logs'\n",
    "summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "tf.summary.trace_on(graph=True) \n",
    "print(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', \n",
    "                                       'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "shakespeare_lines = tf.data.TextLineDataset(path_to_file)\n",
    "shakespeare_tensor = tf.strings.join(list(iter(shakespeare_lines)),\n",
    "                                     separator='\\n')\n",
    "shakespeare_str = shakespeare_tensor.numpy().decode()\n",
    "print(shakespeare_str[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_SEQ_SIZE = 64\n",
    "MAX_SEQ_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def substr_generator():\n",
    "    text_size = len(shakespeare_str)\n",
    "    while True:\n",
    "        index = random.randint(0, text_size - MAX_SEQ_SIZE)\n",
    "        size = random.randint(MIN_SEQ_SIZE, MAX_SEQ_SIZE)\n",
    "        yield tf.strings.substr(shakespeare_tensor, index, size)\n",
    "\n",
    "random.seed(0)\n",
    "substrs_ds = tf.data.Dataset.from_generator(substr_generator, tf.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'alters.\\n\\nPERDITA:\\nOne of these is true:\\nI think affliction may subdue the cheek,\\nBut not take in the mind.\\n\\nCAMILLO:\\n', shape=(), dtype=string)\n",
      "tf.Tensor(b'w together: grant that, and tell me,\\nIn peace what each of them by the other lose,\\nThat they comb', shape=(), dtype=string)\n",
      "tf.Tensor(b'hs up,\\nAfter our great good cheer. Pray you, sit down;\\nFor now we sit to chat as well as eat.\\n\\nPETRUCHIO:\\nNothing but sit and ', shape=(), dtype=string)\n",
      "tf.Tensor(b'on\\nThat does affect it. Once more, fare you well.\\n\\nANGELO:\\nThe heavens give safety to your purposes!\\n\\n', shape=(), dtype=string)\n",
      "tf.Tensor(b'o do?\\n\\nPETRUCHIO:\\nNot her that chides, sir, at any hand, I pray.\\n\\nTRANIO:\\nI love no chiders, sir. Biondello, ', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for x in substrs_ds.take(5):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language = set(shakespeare_str)\n",
    "language_size = len(language)\n",
    "language_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 8\n",
    "hidden_size = 256\n",
    "num_heads = 8\n",
    "max_positions=128\n",
    "\n",
    "num_special_tokens = 2\n",
    "pad_token = language_size + 1\n",
    "mask_token = language_size + 2\n",
    "# mask hyperparams from https://arxiv.org/pdf/1810.04805.pdf\n",
    "prob_mask=0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = transformers.BertConfig(\n",
    "    vocab_size_or_config_json_file=language_size,\n",
    "    type_vocab_size=num_special_tokens, # The [MASK] and [PAD] token\n",
    "    hidden_size=hidden_size,\n",
    "    intermediate_size=hidden_size*2,\n",
    "    num_hidden_layers=num_layers,\n",
    "    num_attention_heads=num_heads\n",
    ")\n",
    "model = transformers.TFBertForMaskedLM(model_config)\n",
    "# model_config = transformers.OpenAIGPTConfig(\n",
    "#     vocab_size_or_config_json_file=language_size,\n",
    "#     type_vocab_size=num_special_tokens,\n",
    "#     n_positions=max_positions,\n",
    "#     n_ctx=max_positions,\n",
    "#     n_embd=hidden_size,\n",
    "#     n_layer=num_layers,\n",
    "#     n_head=num_heads\n",
    "# )\n",
    "# model = transformers.TFOpenAIGPTLMHeadModel(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "train_acc = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(language)}\n",
    "idx2char = dict(enumerate(language))\n",
    "\n",
    "\n",
    "def encode(text_tensor):\n",
    "    encoded_text = [\n",
    "        char2idx[c] for c in text_tensor.numpy().decode()\n",
    "    ]\n",
    "    \n",
    "    def _maybe_mask(x):\n",
    "        if random.random() < prob_mask:\n",
    "            return mask_token\n",
    "        return x\n",
    "    \n",
    "    masked = [\n",
    "        _maybe_mask(x) for x in encoded_text\n",
    "    ]\n",
    "    return masked, encoded_text\n",
    "\n",
    "\n",
    "def tf_encode(text_tensor):\n",
    "    return tf.py_function(encode, [text_tensor], [tf.int32, tf.int32])\n",
    "\n",
    "\n",
    "dataset = substrs_ds.map(tf_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp, tar = next(iter(dataset))\n",
    "inp = tf.expand_dims(inp, 0)\n",
    "tar = tf.expand_dims(tar, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[67, 67, 67, 21, 62, 51, 49, 51, 67, 67, 67, 32, 53, 51, 45, 53,\n",
       "          5, 58, 42, 58, 60, 67, 54,  5, 67,  6, 53, 46, 34, 53, 29,  4,\n",
       "         38, 42, 67,  2, 46, 64, 67, 61, 67, 21, 17, 43, 12, 25, 53, 67,\n",
       "         43,  7, 52, 17, 55, 23, 67, 21, 23,  5, 46, 34, 67, 46, 34, 67,\n",
       "          6, 67, 67, 67, 67, 51,  6,  6, 58, 42, 10, 16, 16, 18, 38, 42,\n",
       "         34, 58, 20, 53, 31, 46, 19, 67, 53, 64, 58, 67, 19, 58, 53, 51,\n",
       "          2, 67, 46, 64, 58, 20, 21, 54, 58, 67, 67, 38, 34, 67, 67,  6,\n",
       "         67, 64, 35, 53, 46,  3, 53, 67, 58, 48, 42, 58, 67, 67, 16, 16]])>,\n",
       " 67,\n",
       " <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       " array([[55, 23, 10, 21, 62, 51, 49, 51, 45, 20, 53, 32, 53, 51, 45, 53,\n",
       "          5, 58, 42, 58, 60, 21, 54,  5, 51,  6, 53, 46, 34, 53, 29,  4,\n",
       "         38, 42, 53,  2, 46, 64, 64, 61, 21, 21, 17, 43, 12, 25, 53, 30,\n",
       "         43,  7, 52, 17, 55, 23, 10, 21, 23,  5, 46, 34, 53, 46, 34, 53,\n",
       "          6,  5, 58, 53, 45, 51,  6,  6, 58, 42, 10, 16, 16, 18, 38, 42,\n",
       "         34, 58, 20, 53, 31, 46, 19, 58, 53, 64, 58, 51, 19, 58, 53, 51,\n",
       "          2,  5, 46, 64, 58, 20, 21, 54, 58, 53, 45, 38, 34,  6, 53,  6,\n",
       "         51, 64, 35, 53, 46,  3, 53, 34, 58, 48, 42, 58,  6, 10, 16, 16]])>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp, mask_token, tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,\n",
       "        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n",
       "        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0]])>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_type_ids = tf.cast(inp == mask_token, tf.int32)\n",
    "token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, *_ = model(inp, token_type_ids=token_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(30,), dtype=int32, numpy=\n",
       "array([55, 23, 10, 45, 20, 53, 21, 51, 53, 64, 21, 30, 10, 53, 53,  5, 58,\n",
       "       53, 45, 58, 51,  5, 53, 45,  6, 53, 51, 34,  6, 10])>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tar[inp == mask_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=4.305786>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(tar[inp == mask_token], logits[inp == mask_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_masked_lm\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  4431360   \n",
      "_________________________________________________________________\n",
      "mlm___cls (TFBertMLMHead)    multiple                  215105    \n",
      "=================================================================\n",
      "Total params: 4,497,729\n",
      "Trainable params: 4,497,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "train_dataset = dataset.cache()\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "padded_shape = [MAX_SEQ_SIZE]\n",
    "train_dataset = train_dataset.padded_batch(\n",
    "    BATCH_SIZE, \n",
    "    padded_shapes=(padded_shape, padded_shape), \n",
    "    padding_values=(pad_token, pad_token)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp, tar = next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([8, 128]), TensorShape([8, 128]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp.shape, tar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=4.2396054>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding_mask = tf.cast(inp != pad_token, tf.int32)\n",
    "token_type_ids = tf.cast(inp == mask_token, tf.int32)\n",
    "logits, *_ = model(inp, \n",
    "                   attention_mask=padding_mask,\n",
    "                   token_type_ids=token_type_ids)\n",
    "loss_fn(tar[inp == mask_token], \n",
    "        logits[inp == mask_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: tf.keras.Model,\n",
    "               optimizer: tf.keras.optimizers.Optimizer,\n",
    "               pad_token: int,\n",
    "               mask_token: int,\n",
    "               inp: tf.Tensor,\n",
    "               tar: tf.Tensor):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        attention_mask = tf.cast(inp != pad_token, tf.int32)\n",
    "        token_type_ids = tf.cast(inp == mask_token, tf.int32)\n",
    "        logits, *_ = model(inp, \n",
    "                           attention_mask=attention_mask,\n",
    "                           token_type_ids=token_type_ids,\n",
    "                           training=True)\n",
    "        loss = loss_fn(tar[inp == mask_token], \n",
    "                       logits[inp == mask_token])\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_acc(tar[inp == mask_token], \n",
    "              logits[inp == mask_token])\n",
    "    train_loss(loss)\n",
    "    \n",
    "    return logits, loss\n",
    "\n",
    "\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(BATCH_SIZE, MAX_SEQ_SIZE), dtype=tf.int32),\n",
    "    tf.TensorSpec(shape=(BATCH_SIZE, MAX_SEQ_SIZE), dtype=tf.int32),\n",
    "]\n",
    "\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step_tf(inp, tar):\n",
    "    train_step(model, optimizer, \n",
    "               pad_token, mask_token, \n",
    "               inp, tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "BATCHES_IN_EPOCH = 250\n",
    "\n",
    "checkpoint_path = f'{OUTPUTS_DIR}/ckpts'\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=model,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_masked_lm/bert/pooler/dense/kernel:0', 'tf_bert_for_masked_lm/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dylan\\Miniconda3\\envs\\xai-it\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_masked_lm/bert/pooler/dense/kernel:0', 'tf_bert_for_masked_lm/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dylan\\Miniconda3\\envs\\xai-it\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 4.2353 Accuracy 0.0246\n",
      "Epoch 1 Batch 50 Loss 3.7593 Accuracy 0.1298\n",
      "Epoch 1 Batch 100 Loss 3.6136 Accuracy 0.1395\n",
      "Epoch 1 Batch 150 Loss 3.5410 Accuracy 0.1420\n",
      "Epoch 1 Batch 200 Loss 3.4961 Accuracy 0.1440\n",
      "Epoch 1 Batch 250 Loss 3.4655 Accuracy 0.1456\n",
      "Epoch 1 Loss 3.4655 Accuracy 0.1456\n",
      "Time taken for 1 epoch: 97.40566444396973 secs\n",
      "\n",
      "Epoch 2 Batch 0 Loss 3.6363 Accuracy 0.1048\n",
      "Epoch 2 Batch 50 Loss 3.3376 Accuracy 0.1497\n",
      "Epoch 2 Batch 100 Loss 3.3369 Accuracy 0.1546\n",
      "Epoch 2 Batch 150 Loss 3.3315 Accuracy 0.1540\n",
      "Epoch 2 Batch 200 Loss 3.3350 Accuracy 0.1524\n",
      "Epoch 2 Batch 250 Loss 3.3316 Accuracy 0.1533\n",
      "Epoch 2 Loss 3.3316 Accuracy 0.1533\n",
      "Time taken for 1 epoch: 70.54202628135681 secs\n",
      "\n",
      "Epoch 3 Batch 0 Loss 3.2630 Accuracy 0.1333\n",
      "Epoch 3 Batch 50 Loss 3.3220 Accuracy 0.1502\n",
      "Epoch 3 Batch 100 Loss 3.3066 Accuracy 0.1524\n",
      "Epoch 3 Batch 150 Loss 3.3191 Accuracy 0.1536\n",
      "Manual interrupt\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for epoch in range(EPOCHS):\n",
    "        start = time.time()\n",
    "\n",
    "        train_loss.reset_states()\n",
    "        train_acc.reset_states()\n",
    "\n",
    "        for batch, (inp, tar) in enumerate(train_dataset):\n",
    "            train_step_tf(inp, tar)\n",
    "\n",
    "            if batch % 50 == 0:\n",
    "                print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "                    epoch + 1, batch, train_loss.result(), train_acc.result()))\n",
    "\n",
    "                with summary_writer.as_default():\n",
    "                    tf.summary.scalar('loss', train_loss.result(), \n",
    "                                      step=epoch)\n",
    "                    tf.summary.scalar('accuracy', train_acc.result(), \n",
    "                                      step=epoch)\n",
    "\n",
    "            if batch >= BATCHES_IN_EPOCH:\n",
    "                break\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            ckpt_save_path = ckpt_manager.save()\n",
    "            print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                                 ckpt_save_path))\n",
    "\n",
    "        print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
    "                                                             train_loss.result(), \n",
    "                                                             train_acc.result()))\n",
    "\n",
    "        print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))\n",
    "except KeyboardInterrupt:\n",
    "    print('Manual interrupt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def to_string(x: tf.Tensor) -> str:\n",
    "    return ''.join(idx2char[i] for i in x.numpy())\n",
    "\n",
    "def generate_text(\n",
    "    model: tf.keras.Model,\n",
    "    start: str,\n",
    "    mask_token: int,\n",
    "    temperature = 1.0,\n",
    "    steps=20,\n",
    "    print_process=True\n",
    "):\n",
    "    if print_process:\n",
    "        print(start)\n",
    "        print('------')\n",
    "    \n",
    "    inp = [char2idx[c] for c in start]\n",
    "    \n",
    "    for _ in range(steps):\n",
    "        inp_with_mask_at_end = tf.concat([inp, [mask_token]], 0)\n",
    "        inp_with_mask_at_end = tf.expand_dims(inp_with_mask_at_end, 0)\n",
    "        token_type_ids = tf.cast(inp_with_mask_at_end == mask_token, tf.int32)\n",
    "        outputs = model(inp_with_mask_at_end, token_type_ids=token_type_ids)\n",
    "        \n",
    "        next_chr_preds = outputs[0][0][-1]\n",
    "        next_chr_preds = next_chr_preds / temperature\n",
    "        next_chr_preds = tf.expand_dims(next_chr_preds, 0)\n",
    "        predicted_id = tf.random.categorical(next_chr_preds, \n",
    "                                             num_samples=1,\n",
    "                                             dtype=tf.int32)[-1, 0]\n",
    "\n",
    "        inp = tf.concat([inp, tf.reshape(predicted_id, (1,))], 0)\n",
    "        \n",
    "        if print_process:\n",
    "            print(to_string(inp))\n",
    "            print('------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "------\n",
      "ROMEO:d\n",
      "------\n",
      "ROMEO:di\n",
      "------\n",
      "ROMEO:di \n",
      "------\n",
      "ROMEO:di l\n",
      "------\n",
      "ROMEO:di lo\n",
      "------\n",
      "ROMEO:di lor\n",
      "------\n",
      "ROMEO:di lorv\n",
      "------\n",
      "ROMEO:di lorvs\n",
      "------\n",
      "ROMEO:di lorvs \n",
      "------\n",
      "ROMEO:di lorvs G\n",
      "------\n",
      "ROMEO:di lorvs G \n",
      "------\n",
      "ROMEO:di lorvs G s\n",
      "------\n",
      "ROMEO:di lorvs G se\n",
      "------\n",
      "ROMEO:di lorvs G seh\n",
      "------\n",
      "ROMEO:di lorvs G seho\n",
      "------\n",
      "ROMEO:di lorvs G seho \n",
      "------\n",
      "ROMEO:di lorvs G seho u\n",
      "------\n",
      "ROMEO:di lorvs G seho uR\n",
      "------\n",
      "ROMEO:di lorvs G seho uRs\n",
      "------\n",
      "ROMEO:di lorvs G seho uRs\n",
      "\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "generate_text(model, 'ROMEO:', mask_token, print_process=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_weights(attention, sentence, layer):\n",
    "    fig = plt.figure(figsize=(16, 2))\n",
    "\n",
    "    attention = tf.squeeze(attention[layer], axis=0)\n",
    "\n",
    "    for head in range(attention.shape[0]):\n",
    "        ax = fig.add_subplot(2, 4, head+1)\n",
    "\n",
    "        # plot the attention weights\n",
    "        ax.matshow(attention[head], \n",
    "                   cmap='viridis')\n",
    "\n",
    "        fontdict = {'fontsize': 10}\n",
    "\n",
    "        ax.set_xticks(range(len(sentence)+2))\n",
    "\n",
    "        ax.set_xticklabels(\n",
    "            ['-']+[idx2char[i] for i in sentence]+['-'], \n",
    "            fontdict=fontdict)\n",
    "\n",
    "        ax.set_yticklabels([])\n",
    "\n",
    "        ax.set_xlabel('Head {}'.format(head+1))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_attention_weights(attention_weights, x.numpy(), 'decoder_layer1_block2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
